{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        
      ],
      "metadata": {
        "id": "E5GwL-ZtBXaD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5mXSIHhkLDM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import os\n",
        "import time\n",
        "import re # Import regular expressions for parsing\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ku5yvF1_ovXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load data"
      ],
      "metadata": {
        "id": "noDHVz9aotv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 1. Load All Data ---\n",
        "try:\n",
        "    df_train_small_full = pd.read_csv(\"small_train_data.csv\") # 100k rows\n",
        "    df_train_large_full = pd.read_csv(\"large_train_data.csv\") # 500k rows\n",
        "    df_test_full = pd.read_csv(\"test_without_response_variable.csv\") # 60k rows\n",
        "except Exception as e:\n",
        "    print(f\"Error loading files. Make sure all 3 files are uploaded:\")\n",
        "    print(\"small_train_data.csv, large_train_data.csv, test_without_response_variable.csv\")\n",
        "    print(e)\n",
        "    raise\n",
        "\n",
        "# --- 2. Combine and Sample Training Data ---\n",
        "# Stack the small and large training sets\n",
        "df_train_combined = pd.concat([df_train_small_full, df_train_large_full], ignore_index=True)\n",
        "print(f\"Total combined training rows: {len(df_train_combined)}\")\n",
        "\n",
        "# Let's use 40% of the total combined data\n",
        "SAMPLE_FRACTION = 0.4\n",
        "df_train = df_train_combined.sample(frac=SAMPLE_FRACTION, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# --- 3. Create & Impute Target Variable (log_price) ---\n",
        "TARGET_VARIABLE = 'log_price'\n",
        "# Calculate log_price, allowing -inf (from log(0)) or NaN (from log(-))\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    df_train[TARGET_VARIABLE] = np.log(df_train['price'])\n",
        "\n",
        "# Find all valid log_price values\n",
        "valid_log_prices = df_train[np.isfinite(df_train[TARGET_VARIABLE])][TARGET_VARIABLE]\n",
        "\n",
        "# Calculate the mean of only the *valid* log_prices\n",
        "log_price_mean = valid_log_prices.mean()\n",
        "\n",
        "# Replace all invalid log_prices (NaN, inf, -inf) with the mean\n",
        "df_train[TARGET_VARIABLE] = df_train[TARGET_VARIABLE].replace([np.inf, -np.inf, np.nan], log_price_mean)\n",
        "\n",
        "\n",
        "# --- 4. Data Setup ---\n",
        "X_train = df_train.drop(columns=['price', TARGET_VARIABLE], errors='ignore')\n",
        "y_train = df_train[TARGET_VARIABLE]\n",
        "# full test set\n",
        "#X_test = df_test_full\n",
        "# Use a 1/10th sample of the test set\n",
        "# We save the original sampled dataframe (df_test_small) to check results later\n",
        "df_test_small = df_test_full.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
        "X_test = df_test_small\n",
        "\n",
        "\n",
        "print(f\"Using {len(X_train)} sampled training observations.\")\n",
        "print(f\"Using {len(X_test)} test observations.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBdsmFZNclGe",
        "outputId": "0a75cc1a-6bdf-4706-dfc5-958ca826a95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total combined training rows: 600000\n",
            "Using 240000 sampled training observations.\n",
            "Using 6000 test observations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "Y9eCaoetlPMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Helper Function to Parse Strings ---\n",
        "def parse_string_to_num(series):\n",
        "    return pd.to_numeric(\n",
        "        series.astype(str).str.extract(r'([\\d\\.]+)', expand=False),\n",
        "        errors='coerce'\n",
        "    )\n",
        "\n",
        "# --- 2. Main Advanced Preprocessing Function ---\n",
        "def advanced_preprocess_features(df_in):\n",
        "    df = df_in.copy()\n",
        "    df['power_num'] = parse_string_to_num(df['power'])\n",
        "    df['torque_num'] = parse_string_to_num(df['torque'])\n",
        "    df['back_legroom_num'] = parse_string_to_num(df['back_legroom'])\n",
        "    df['front_legroom_num'] = parse_string_to_num(df['front_legroom'])\n",
        "    df['fuel_tank_volume_num'] = parse_string_to_num(df['fuel_tank_volume'])\n",
        "    df['height_num'] = parse_string_to_num(df['height'])\n",
        "    df['length_num'] = parse_string_to_num(df['length'])\n",
        "    df['width_num'] = parse_string_to_num(df['width'])\n",
        "    df['wheelbase_num'] = parse_string_to_num(df['wheelbase'])\n",
        "    df['engine_cylinders_num'] = pd.to_numeric(\n",
        "        df['engine_cylinders'].astype(str).str.replace(r'\\D', ''),\n",
        "        errors='coerce'\n",
        "    )\n",
        "    df['maximum_seating_num'] = pd.to_numeric(\n",
        "        df['maximum_seating'].astype(str).str.replace(r'\\D', ''),\n",
        "        errors='coerce'\n",
        "    )\n",
        "    df['major_options_count'] = df['major_options'].str.count(\"'\").fillna(0) // 2\n",
        "    current_year = 2024\n",
        "    df['age'] = current_year - df['year']\n",
        "    df['mileage_per_year'] = df['mileage'] / (df['age'] + 1)\n",
        "    return df\n",
        "\n",
        "# --- 3. Apply Advanced Preprocessing ---\n",
        "print(\"Starting advanced preprocessing...\")\n",
        "X_train = advanced_preprocess_features(X_train)\n",
        "X_test = advanced_preprocess_features(X_test)\n",
        "print(\"Advanced preprocessing finished.\")\n",
        "\n",
        "# --- 4. Define ALL Features for the Model ---\n",
        "NUMERICAL_FEATURES_ORIG = [\n",
        "    'year', 'mileage', 'horsepower', 'city_fuel_economy',\n",
        "    'highway_fuel_economy', 'engine_displacement'\n",
        "]\n",
        "NUMERICAL_FEATURES_NEW = [\n",
        "    'power_num', 'torque_num', 'back_legroom_num', 'front_legroom_num',\n",
        "    'fuel_tank_volume_num', 'height_num', 'length_num', 'width_num',\n",
        "    'wheelbase_num', 'engine_cylinders_num', 'maximum_seating_num',\n",
        "    'major_options_count', 'age', 'mileage_per_year'\n",
        "]\n",
        "CATEGORICAL_FEATURES = [\n",
        "    'make_name', 'body_type', 'fuel_type', 'transmission_display'\n",
        "]\n",
        "all_numerical_features = NUMERICAL_FEATURES_ORIG + NUMERICAL_FEATURES_NEW\n",
        "\n",
        "# =======================================================================\n",
        "# --- 5. Imputation (Fill NaNs) - ROBUST FIX for X ---\n",
        "print(\"Starting robust imputation...\")\n",
        "for col in all_numerical_features:\n",
        "    mean_val = X_train[col].mean()\n",
        "\n",
        "    # Check if the mean itself is NaN (the whole column was NaN)\n",
        "    if pd.isna(mean_val):\n",
        "        mean_val = 0 # Default to 0\n",
        "\n",
        "    X_train[col] = X_train[col].fillna(mean_val)\n",
        "    X_test[col] = X_test[col].fillna(mean_val)\n",
        "\n",
        "for col in CATEGORICAL_FEATURES:\n",
        "    X_train[col] = X_train[col].fillna('Missing')\n",
        "    X_test[col] = X_test[col].fillna('Missing')\n",
        "print(\"Imputation finished.\")\n",
        "# =======================================================================\n",
        "\n",
        "# --- 6. Preprocessing Pipeline (Scaling & OHE) ---\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), all_numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "X_train_processed = X_train_processed.toarray()\n",
        "X_test_processed = X_test_processed.toarray()\n",
        "INPUT_DIM = X_train_processed.shape[1]\n",
        "\n",
        "print(f\"\\n--- Preprocessing Complete ---\")\n",
        "print(f\"Total input features for the NN (after parsing and OHE): {INPUT_DIM}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5V8UXO5oFuL",
        "outputId": "38ecd130-de52-4b72-e5a6-bfdc70158098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting advanced preprocessing...\n",
            "Advanced preprocessing finished.\n",
            "Starting robust imputation...\n",
            "Imputation finished.\n",
            "\n",
            "--- Preprocessing Complete ---\n",
            "Total input features for the NN (after parsing and OHE): 122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Hyperparameter Tuning Loop"
      ],
      "metadata": {
        "id": "FddYuFCWlZzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Define Model & Training Parameters ---\n",
        "nodes_to_test = [64, 128, 256, 512] # The list of layer sizes to try\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100 # Use a high number with Early Stopping\n",
        "tuning_results = {}\n",
        "\n",
        "# --- 2. Split Data (Once) ---\n",
        "# We use the same validation split for all tuning runs\n",
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(\n",
        "    X_train_processed, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- 3. Tuning Loop ---\n",
        "print(f\"\\n--- Starting NN Hyperparameter Tuning Loop ---\")\n",
        "print(f\"Training on {len(X_train_fit)} samples, Validating on {len(X_val_fit)} samples.\")\n",
        "\n",
        "for nodes in nodes_to_test:\n",
        "    print(f\"\\n--- Testing HIDDEN_NODES = {nodes} ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Build the Model ---\n",
        "    model = Sequential([\n",
        "        Dense(nodes, activation='relu', input_shape=(INPUT_DIM,)),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='mse')\n",
        "\n",
        "    # --- Train the Model ---\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    history = model.fit(\n",
        "        X_train_fit, y_train_fit,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=(X_val_fit, y_val_fit),\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0 # Set to 0 to keep the log clean\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # --- Get Best MSE and R-squared ---\n",
        "    # Find the best MSE (val_loss) from this run\n",
        "    best_mse = min(history.history['val_loss'])\n",
        "\n",
        "    # Calculate R-squared on validation set\n",
        "    y_pred_validation = model.predict(X_val_fit).flatten()\n",
        "    r2_val = r2_score(y_val_fit, y_pred_validation)\n",
        "\n",
        "    # Store results\n",
        "    tuning_results[nodes] = {'mse': best_mse, 'r2': r2_val, 'time': end_time - start_time}\n",
        "    print(f\"Finished in {end_time - start_time:.2f}s. Best Validation MSE: {best_mse:.6f}, Validation R2: {r2_val:.6f}\")\n",
        "\n",
        "# --- 4. Report Final Results ---\n",
        "print(\"\\n--- Tuning Complete: Results ---\")\n",
        "print(\"Nodes | Best Val MSE (Lower is Better) | Val R-Squared (Higher is Better)\")\n",
        "print(\"------|--------------------------------|---------------------------------\")\n",
        "\n",
        "best_nodes = 0\n",
        "best_mse = float('inf')\n",
        "\n",
        "for nodes, metrics in tuning_results.items():\n",
        "    print(f\"{nodes:<5} | {metrics['mse']:<30.6f} | {metrics['r2']:<30.6f}\")\n",
        "    if metrics['mse'] < best_mse:\n",
        "        best_mse = metrics['mse']\n",
        "        best_nodes = nodes\n",
        "\n",
        "print(\"\\n--- Recommendation ---\")\n",
        "print(f\"The best performing model used {best_nodes} nodes in the hidden layer.\")\n",
        "print(f\"It achieved a Validation MSE of: {best_mse:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-Fh6RculX3a",
        "outputId": "80146732-90ef-43b0-f623-28048c908a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting NN Hyperparameter Tuning Loop ---\n",
            "Training on 144000 samples, Validating on 36000 samples.\n",
            "\n",
            "--- Testing HIDDEN_NODES = 64 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Finished in 473.37s. Best Validation MSE: 0.011442, Validation R2: 0.925758\n",
            "\n",
            "--- Testing HIDDEN_NODES = 128 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Finished in 377.54s. Best Validation MSE: 0.011143, Validation R2: 0.927696\n",
            "\n",
            "--- Testing HIDDEN_NODES = 256 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Finished in 525.05s. Best Validation MSE: 0.010832, Validation R2: 0.929716\n",
            "\n",
            "--- Testing HIDDEN_NODES = 512 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Finished in 332.62s. Best Validation MSE: 0.011939, Validation R2: 0.922530\n",
            "\n",
            "--- Tuning Complete: Results ---\n",
            "Nodes | Best Val MSE (Lower is Better) | Val R-Squared (Higher is Better)\n",
            "------|--------------------------------|---------------------------------\n",
            "64    | 0.011442                       | 0.925758                      \n",
            "128   | 0.011143                       | 0.927696                      \n",
            "256   | 0.010832                       | 0.929716                      \n",
            "512   | 0.011939                       | 0.922530                      \n",
            "\n",
            "--- Recommendation ---\n",
            "The best performing model used 256 nodes in the hidden layer.\n",
            "It achieved a Validation MSE of: 0.010832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Final Winning Model"
      ],
      "metadata": {
        "id": "0Va2HHE_ljzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# --- 1. Define Winning Hyperparameters ---\n",
        "BEST_NODES = 256               # The winner from tuning\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 150\n",
        "L2_DECAY = 0.0001\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "# --- 2. Split Data (for training the final model) ---\n",
        "# We use the same split to be consistent\n",
        "X_train_fit, X_val_fit, y_train_fit, y_val_fit = train_test_split(\n",
        "    X_train_processed, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- 3. Build the Final Model ---\n",
        "print(f\"\\n--- Building the Final Winning Model (Nodes = {BEST_NODES}) ---\")\n",
        "final_model = Sequential([\n",
        "    Dense(\n",
        "        BEST_NODES,\n",
        "        activation='relu',\n",
        "        kernel_regularizer=l2(L2_DECAY),\n",
        "        input_shape=(INPUT_DIM,)\n",
        "    ),\n",
        "    BatchNormalization(),\n",
        "    Dropout(DROPOUT_RATE),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# --- 4. Compile the Final Model ---\n",
        "final_model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='mse')\n",
        "\n",
        "# --- 5. Define Callbacks ---\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "# --- 6. Train the Final Model ---\n",
        "print(f\"Training final model on {len(X_train_fit)} samples...\")\n",
        "start_time = time.time()\n",
        "history = final_model.fit(\n",
        "    X_train_fit, y_train_fit,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val_fit, y_val_fit),\n",
        "    callbacks=[early_stop, lr_scheduler],\n",
        "    verbose=1 # Set to 1 to watch it train one last time\n",
        ")\n",
        "end_time = time.time()\n",
        "\n",
        "best_mse = min(history.history['val_loss'])\n",
        "print(f\"\\n--- Final Model Trained in {end_time - start_time:.2f}s ---\")\n",
        "print(f\"Final Model Best Validation MSE: {best_mse:.6f}\")\n",
        "print(\"The 'final_model' variable is now ready for prediction.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvnWzuQMlY7p",
        "outputId": "15181a5d-5c59-40ef-8485-5713c9560871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Building the Final Winning Model (Nodes = 256) ---\n",
            "Training final model on 192000 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - loss: 8.4899 - val_loss: 0.0375 - learning_rate: 0.0010\n",
            "Epoch 2/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.1392 - val_loss: 0.0348 - learning_rate: 0.0010\n",
            "Epoch 3/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0783 - val_loss: 0.0202 - learning_rate: 0.0010\n",
            "Epoch 4/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0645 - val_loss: 0.0203 - learning_rate: 0.0010\n",
            "Epoch 5/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - loss: 0.0598 - val_loss: 0.0220 - learning_rate: 0.0010\n",
            "Epoch 6/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0600 - val_loss: 0.0223 - learning_rate: 0.0010\n",
            "Epoch 7/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0558 - val_loss: 0.0165 - learning_rate: 0.0010\n",
            "Epoch 8/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0540 - val_loss: 0.0191 - learning_rate: 0.0010\n",
            "Epoch 9/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0523 - val_loss: 0.0258 - learning_rate: 0.0010\n",
            "Epoch 10/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0508 - val_loss: 0.0323 - learning_rate: 0.0010\n",
            "Epoch 11/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0487 - val_loss: 0.0185 - learning_rate: 0.0010\n",
            "Epoch 12/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0485 - val_loss: 0.0167 - learning_rate: 0.0010\n",
            "Epoch 13/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0424 - val_loss: 0.0151 - learning_rate: 2.0000e-04\n",
            "Epoch 14/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0409 - val_loss: 0.0146 - learning_rate: 2.0000e-04\n",
            "Epoch 15/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0405 - val_loss: 0.0145 - learning_rate: 2.0000e-04\n",
            "Epoch 16/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0399 - val_loss: 0.0140 - learning_rate: 2.0000e-04\n",
            "Epoch 17/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0397 - val_loss: 0.0154 - learning_rate: 2.0000e-04\n",
            "Epoch 18/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0394 - val_loss: 0.0136 - learning_rate: 2.0000e-04\n",
            "Epoch 19/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.0154 - learning_rate: 2.0000e-04\n",
            "Epoch 20/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0386 - val_loss: 0.0136 - learning_rate: 2.0000e-04\n",
            "Epoch 21/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0383 - val_loss: 0.0147 - learning_rate: 2.0000e-04\n",
            "Epoch 22/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0383 - val_loss: 0.0134 - learning_rate: 2.0000e-04\n",
            "Epoch 23/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0377 - val_loss: 0.0154 - learning_rate: 2.0000e-04\n",
            "Epoch 24/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0375 - val_loss: 0.0145 - learning_rate: 2.0000e-04\n",
            "Epoch 25/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0375 - val_loss: 0.0167 - learning_rate: 2.0000e-04\n",
            "Epoch 26/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0367 - val_loss: 0.0141 - learning_rate: 2.0000e-04\n",
            "Epoch 27/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0368 - val_loss: 0.0133 - learning_rate: 2.0000e-04\n",
            "Epoch 28/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0366 - val_loss: 0.0145 - learning_rate: 2.0000e-04\n",
            "Epoch 29/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0362 - val_loss: 0.0150 - learning_rate: 2.0000e-04\n",
            "Epoch 30/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0358 - val_loss: 0.0139 - learning_rate: 2.0000e-04\n",
            "Epoch 31/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0147 - learning_rate: 2.0000e-04\n",
            "Epoch 32/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0358 - val_loss: 0.0131 - learning_rate: 2.0000e-04\n",
            "Epoch 33/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4ms/step - loss: 0.0348 - val_loss: 0.0139 - learning_rate: 2.0000e-04\n",
            "Epoch 34/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0353 - val_loss: 0.0137 - learning_rate: 2.0000e-04\n",
            "Epoch 35/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0344 - val_loss: 0.0141 - learning_rate: 2.0000e-04\n",
            "Epoch 36/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0344 - val_loss: 0.0138 - learning_rate: 2.0000e-04\n",
            "Epoch 37/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0343 - val_loss: 0.0130 - learning_rate: 2.0000e-04\n",
            "Epoch 38/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0330 - val_loss: 0.0123 - learning_rate: 4.0000e-05\n",
            "Epoch 39/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0326 - val_loss: 0.0123 - learning_rate: 4.0000e-05\n",
            "Epoch 40/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0326 - val_loss: 0.0127 - learning_rate: 4.0000e-05\n",
            "Epoch 41/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0322 - val_loss: 0.0125 - learning_rate: 4.0000e-05\n",
            "Epoch 42/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0320 - val_loss: 0.0124 - learning_rate: 4.0000e-05\n",
            "Epoch 43/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0326 - val_loss: 0.0124 - learning_rate: 4.0000e-05\n",
            "Epoch 44/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0320 - val_loss: 0.0123 - learning_rate: 1.0000e-05\n",
            "Epoch 45/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0319 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 46/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0315 - val_loss: 0.0123 - learning_rate: 1.0000e-05\n",
            "Epoch 47/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0316 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 48/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0318 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 49/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0318 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 50/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0319 - val_loss: 0.0123 - learning_rate: 1.0000e-05\n",
            "Epoch 51/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4ms/step - loss: 0.0315 - val_loss: 0.0122 - learning_rate: 1.0000e-05\n",
            "Epoch 52/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0316 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 53/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0321 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 54/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0316 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 55/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0318 - val_loss: 0.0122 - learning_rate: 1.0000e-05\n",
            "Epoch 56/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0319 - val_loss: 0.0123 - learning_rate: 1.0000e-05\n",
            "Epoch 57/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0312 - val_loss: 0.0123 - learning_rate: 1.0000e-05\n",
            "Epoch 58/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 59/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0315 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 60/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0318 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 61/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0318 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 62/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0319 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 63/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 64/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0314 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 65/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0317 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 66/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 67/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0314 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 68/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0314 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 69/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 70/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0314 - val_loss: 0.0123 - learning_rate: 1.0000e-05\n",
            "Epoch 71/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0317 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 72/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0312 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 73/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0316 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 74/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0312 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 75/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0315 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 76/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0312 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 77/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0316 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 78/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0122 - learning_rate: 1.0000e-05\n",
            "Epoch 79/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0318 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 80/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0125 - learning_rate: 1.0000e-05\n",
            "Epoch 81/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0311 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 82/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0315 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 83/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0311 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 84/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0315 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 85/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0313 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 86/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0311 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 87/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - loss: 0.0315 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 88/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 89/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0312 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 90/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0311 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 91/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0316 - val_loss: 0.0124 - learning_rate: 1.0000e-05\n",
            "Epoch 92/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 93/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0312 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 94/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0309 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 95/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 96/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 97/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0313 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 98/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0309 - val_loss: 0.0124 - learning_rate: 1.0000e-05\n",
            "Epoch 99/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0310 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 100/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - loss: 0.0311 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 101/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0310 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 102/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0312 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 103/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0308 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 104/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - loss: 0.0312 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 105/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0314 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 106/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0311 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 107/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0307 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 108/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0307 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 109/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0310 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 110/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0308 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 111/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0311 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 112/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0310 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 113/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0312 - val_loss: 0.0117 - learning_rate: 1.0000e-05\n",
            "Epoch 114/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0308 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 115/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0303 - val_loss: 0.0121 - learning_rate: 1.0000e-05\n",
            "Epoch 116/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0307 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 117/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0305 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 118/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0305 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 119/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0303 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 120/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0308 - val_loss: 0.0122 - learning_rate: 1.0000e-05\n",
            "Epoch 121/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0312 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 122/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0307 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 123/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0309 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 124/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0307 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 125/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0309 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 126/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.0308 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 127/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0306 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 128/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0305 - val_loss: 0.0117 - learning_rate: 1.0000e-05\n",
            "Epoch 129/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0302 - val_loss: 0.0117 - learning_rate: 1.0000e-05\n",
            "Epoch 130/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0311 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 131/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0309 - val_loss: 0.0117 - learning_rate: 1.0000e-05\n",
            "Epoch 132/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0303 - val_loss: 0.0117 - learning_rate: 1.0000e-05\n",
            "Epoch 133/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0303 - val_loss: 0.0117 - learning_rate: 1.0000e-05\n",
            "Epoch 134/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0306 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 135/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0304 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 136/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0307 - val_loss: 0.0122 - learning_rate: 1.0000e-05\n",
            "Epoch 137/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0306 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 138/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0306 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 139/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0303 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 140/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0307 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "Epoch 141/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0306 - val_loss: 0.0118 - learning_rate: 1.0000e-05\n",
            "Epoch 142/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0307 - val_loss: 0.0122 - learning_rate: 1.0000e-05\n",
            "Epoch 143/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 0.0301 - val_loss: 0.0120 - learning_rate: 1.0000e-05\n",
            "Epoch 144/150\n",
            "\u001b[1m6000/6000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 0.0304 - val_loss: 0.0119 - learning_rate: 1.0000e-05\n",
            "\n",
            "--- Final Model Trained in 2815.33s ---\n",
            "Final Model Best Validation MSE: 0.011686\n",
            "The 'final_model' variable is now ready for prediction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Final Submission CSV"
      ],
      "metadata": {
        "id": "GNgUJk_XxAr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load the FULL Test Set ---\n",
        "X_test_full = df_test_full\n",
        "\n",
        "# --- 2. Apply Advanced Preprocessing to FULL Test Set ---\n",
        "# We must run the same feature engineering and parsing\n",
        "print(\"Applying advanced preprocessing to full test set...\")\n",
        "X_test_full = advanced_preprocess_features(X_test_full)\n",
        "\n",
        "# --- 3. Apply Imputation to FULL Test Set ---\n",
        "# We use the same imputation rules as before\n",
        "print(\"Applying imputation to full test set...\")\n",
        "for col in all_numerical_features:\n",
        "    mean_val = X_train[col].mean() # Use mean from the TRAINING set\n",
        "    if pd.isna(mean_val):\n",
        "        mean_val = 0\n",
        "    X_test_full[col] = X_test_full[col].fillna(mean_val)\n",
        "\n",
        "for col in CATEGORICAL_FEATURES:\n",
        "    X_test_full[col] = X_test_full[col].fillna('Missing')\n",
        "\n",
        "# --- 4. Apply Preprocessor to FULL Test Set ---\n",
        "# Use the fitted preprocessor to transform the test data\n",
        "print(\"Applying fitted preprocessor to full test set...\")\n",
        "X_test_full_processed = preprocessor.transform(X_test_full)\n",
        "X_test_full_processed = X_test_full_processed.toarray()\n",
        "\n",
        "print(f\"Test set shape after processing: {X_test_full_processed.shape}\")\n",
        "\n",
        "# --- 5. Generate Final Predictions ---\n",
        "print(\"Generating final predictions on full test set...\")\n",
        "final_log_price_predictions = final_model.predict(X_test_full_processed).flatten()\n",
        "print(f\"Generated {len(final_log_price_predictions)} predictions.\")\n",
        "\n",
        "# --- 6. Create the Final Submission CSV ---\n",
        "YOUR_ANONYMIZED_NAME = \"my_anonymized_name\"\n",
        "YOUR_STUDENT_ID = \"my_student_id\"\n",
        "# ---------------------------------------------\n",
        "nodes_line = str(BEST_NODES) # This is 256\n",
        "\n",
        "submission_file = \"submission_q1.csv\"\n",
        "try:\n",
        "    with open(submission_file, 'w') as f:\n",
        "        f.write(f\"{YOUR_ANONYMIZED_NAME}\\n\")\n",
        "        f.write(f\"{YOUR_STUDENT_ID}\\n\")\n",
        "        f.write(f\"{nodes_line}\\n\")\n",
        "\n",
        "        for pred in final_log_price_predictions:\n",
        "            f.write(f\"{pred}\\n\")\n",
        "\n",
        "    print(f\"Successfully created '{submission_file}'.\")\n",
        "\n",
        "    # Print a summary to double-check\n",
        "    print(\"\\n--- Submission File Check ---\")\n",
        "    print(f\"Line 1 (Name): {YOUR_ANONYMIZED_NAME}\")\n",
        "    print(f\"Line 2 (ID): {YOUR_STUDENT_ID}\")\n",
        "    print(f\"Line 3 (Nodes): {nodes_line}\")\n",
        "    print(f\"Line 4 (First Pred): {final_log_price_predictions[0]}\")\n",
        "    print(f\"Total Predictions: {len(final_log_price_predictions)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred while writing the file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-L7GWGsljJS",
        "outputId": "1d3e0135-8d63-4570-ced1-54a5fbe1f0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying advanced preprocessing to full test set...\n",
            "Applying imputation to full test set...\n",
            "Applying fitted preprocessor to full test set...\n",
            "Test set shape after processing: (60000, 122)\n",
            "Generating final predictions on full test set...\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Generated 60000 predictions.\n",
            "Successfully created 'submission.csv'.\n",
            "\n",
            "--- Submission File Check ---\n",
            "Line 1 (Name): my_anonymized_name\n",
            "Line 2 (ID): my_student_id\n",
            "Line 3 (Nodes): 256\n",
            "Line 4 (First Pred): 9.701264381408691\n",
            "Total Predictions: 60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation set's $R^2$ and MSE"
      ],
      "metadata": {
        "id": "kq_6Gl3s0ilj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Final Model Performance on Validation Set ---\")\n",
        "\n",
        "# 1. Get predictions for the validation set using the final_model\n",
        "y_pred_validation = final_model.predict(X_val_fit).flatten()\n",
        "\n",
        "# 2. Get the true values (the log_price)\n",
        "y_true_validation = y_val_fit\n",
        "\n",
        "# 3. Calculate R-squared\n",
        "# r2_score(y_true, y_pred)\n",
        "r2_val = r2_score(y_true_validation, y_pred_validation)\n",
        "\n",
        "# Get the MSE from the history object (it's the last recorded val_loss)\n",
        "final_mse = history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"Final Model Validation MSE: {final_mse:.6f}\")\n",
        "print(f\"Final Model Validation R-Squared (R2): {r2_val:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dm-TTow0iMW",
        "outputId": "f015fca0-608c-49cb-ff62-9c64449a0ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Model Performance on Validation Set ---\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Final Model Validation MSE: 0.011930\n",
            "Final Model Validation R-Squared (R2): 0.927570\n"
          ]
        }
      ]
    }
  ]
}
